{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPaKVKrM1QzAC6P4+yX2AhQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Riddick4-droid/Machine_Learning-Pt/blob/main/Ad_Clicks_Logistic_Regression_Scikit_learn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LOGISTIC REGRESSION FOR CLICK-THROUGH AD PREDICTIONS"
      ],
      "metadata": {
        "id": "_i7Js_J2axJs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To understand the concept of logistic regression please see my notebook\n",
        "on building `logistic regression from scratch`."
      ],
      "metadata": {
        "id": "taPPlsDecClv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPlR8dZRZxIh"
      },
      "outputs": [],
      "source": [
        "##make basic imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from timeit import default_timer as timer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#i will use some functions from the scratch notebook\n",
        "\n",
        "#calculate the outputs as sigmoid values between 0 and 1\n",
        "def sigmoid(x):\n",
        "    #the function simply takes in an input value and returns a computed sigmoid of the value\n",
        "    #the resulting values usually fall between 0 and 1 as the sigmoid function is a normalization function\n",
        "    return 1.0 / (1+np.exp(-x))\n",
        "\n",
        "#compute predictions-used in the train function\n",
        "def compute_predictions(x,weights):\n",
        "    #this function computes the prediction y_hat based on weights(coefficients)\n",
        "    z = np.dot(x,weights)\n",
        "\n",
        "    #since we are using logistic regression\n",
        "    #all outputs must be between 0 and 1\n",
        "    sig = sigmoid(z)\n",
        "\n",
        "    #return\n",
        "    return sig\n",
        "\n",
        "#function for weight updates-used in training\n",
        "def update_weights(x,y,weights,lr):\n",
        "    #lets get the models prediction\n",
        "    predictions = compute_predictions(weights=weights,x=x)\n",
        "\n",
        "    #get weight changes\n",
        "    #remember that the formual is usually y_hat = np.dot(x,w.T)\n",
        "    #but we use change of subject to find the weights.this is\n",
        "    change_in_weights = np.dot(x.T,y-predictions)\n",
        "\n",
        "    #get sample size\n",
        "    m = y.shape[0]\n",
        "\n",
        "    #weight update\n",
        "    weights += lr /float(m)*change_in_weights\n",
        "\n",
        "    return weights\n",
        "\n",
        "#for computing the cost function\n",
        "def compute_cost(x,y,weights):\n",
        "    #this function calculates the cost function\n",
        "    #the cost is usually a measure of how wrong the model is\n",
        "\n",
        "    predictions = compute_predictions(x,weights)\n",
        "\n",
        "    #cost\n",
        "    cost = np.mean(-y * np.log(predictions) - (1-y)*np.log(1-predictions))\n",
        "\n",
        "    #return\n",
        "    return cost\n",
        "\n",
        "#the train function to get weights\n",
        "#i will spice this up with a timer module\n",
        "def train_logistic_regression(x,y,num_iter,lr,fit_intercept=False):\n",
        "    start = timer()\n",
        "    if fit_intercept:\n",
        "        intercept = np.ones((x.shape[0],1))\n",
        "        x = np.hstack((intercept,x))\n",
        "    weights = np.zeros(x.shape[1])\n",
        "\n",
        "    #update weights iteratively\n",
        "    for iteration in range(num_iter):\n",
        "        weights = update_weights(x,y,weights,lr)\n",
        "\n",
        "        #check the cost for every 50 iterations\n",
        "        if iteration % 50 == 0:\n",
        "            print(f'iter:{iteration}---{round(compute_cost(x,y,weights),3)}')\n",
        "    end = timer()\n",
        "    print(f'trained for {(end-start):.5f}s for {num_iter} iterations')\n",
        "    return weights\n",
        "\n",
        "#make predictions or inference\n",
        "def predict(x,weights):\n",
        "    #we know by now that y_hat is\n",
        "    #based on dot product between\n",
        "    #the weights and the x\n",
        "    if x.shape[1] == weights.shape[0]-1: #we ensure there is a shape match by including an intercept\n",
        "        intercept = np.ones((x.shape[0],1))\n",
        "        x = np.hstack((intercept,x))\n",
        "    return compute_predictions(x,weights)"
      ],
      "metadata": {
        "id": "N76UAZ5BdJJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##get the data\n",
        "import kagglehub\n",
        "path = kagglehub.dataset_download(\"marius2303/ad-click-prediction-dataset\")\n",
        "\n",
        "#print path\n",
        "print(path)"
      ],
      "metadata": {
        "id": "EftEhp4QdrGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dc48928a"
      },
      "source": [
        "import os\n",
        "\n",
        "# List the contents of the downloaded directory\n",
        "print(os.listdir(path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d021f69f"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# check in the directory is 'ad_click_dataset.csv'\n",
        "df = pd.read_csv(os.path.join(path, 'ad_click_dataset.csv'))\n",
        "\n",
        "# Display the first 5 rows of the DataFrame with .head()\n",
        "display(df.head(20))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['id'].nunique()"
      ],
      "metadata": {
        "id": "ViIv12XMpaTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check shape\n",
        "print(f'there are {df.shape[0]} instances and {df.shape[1]} features')"
      ],
      "metadata": {
        "id": "MHK6SQA_gQ6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lets preprocess the data\n",
        "#split the data into target and features\n",
        "x = df.drop(['click','id','full_name','device_type'],axis=1).values\n",
        "\n",
        "y = df['click'].values"
      ],
      "metadata": {
        "id": "LdGz9ODZf4U-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##train test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.1,random_state=42)"
      ],
      "metadata": {
        "id": "qnDvpjymhXEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check shape\n",
        "x_train.shape,y_train.shape"
      ],
      "metadata": {
        "id": "fPC0VMMwhqo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#encode the features\n",
        "enc = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "x_train_enc = enc.fit_transform(x_train)\n",
        "\n",
        "x_test_enc = enc.transform(x_test)"
      ],
      "metadata": {
        "id": "dDOVRLAsh9v4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##expect an increase in feature size due to encoding by creating dummies\n",
        "x_train_enc.shape"
      ],
      "metadata": {
        "id": "a_fHLWl9iOYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##get weights\n",
        "weights = train_logistic_regression(x_train_enc.toarray(),y_train,num_iter=1000,lr=0.01,fit_intercept=True)"
      ],
      "metadata": {
        "id": "TmEFbth3ia7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##make predictions\n",
        "predictions = predict(x_test_enc.toarray(),weights)"
      ],
      "metadata": {
        "id": "fZlE6Q7hi2Po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions.shape"
      ],
      "metadata": {
        "id": "rqqraI8IjkzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##use the more robust roc_auc_score\n",
        "from sklearn.metrics import roc_auc_score,roc_curve\n",
        "\n",
        "print(f'Training samples: {x_train.shape[0]}, AUC on testing set: {roc_auc_score(y_test,predictions):.3f}')"
      ],
      "metadata": {
        "id": "RgEoVwQ7j1Cg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now i try a better way of implementing logistic regression that has the ability to apply SGD\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "#initialize\n",
        "sgd_lr = SGDClassifier(loss='log_loss',penalty=None,fit_intercept=True,max_iter=20,learning_rate='constant',eta0=0.01)"
      ],
      "metadata": {
        "id": "uOscy2XFkUbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, 'log_loss' for the loss parameter indicates that the cost function is log loss,\n",
        "penalty is the regularization term to reduce overfitting, which we will discuss further in\n",
        "the next section, max_iter is the number of iterations, and the remaining two parameters\n",
        "mean the learning rate is 0.01 and unchanged during the course of training. It should\n",
        "be noted that the default learning_rate is 'optimal', where the learning rate slightly\n",
        "decreases as more and more updates are made. This can be beneficial for finding the\n",
        "optimal solution on large datasets"
      ],
      "metadata": {
        "id": "N92Ls7vtlu7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##lets fit our data to the algorithm\n",
        "sgd_lr.fit(x_train_enc.toarray(),y_train)\n",
        "\n",
        "#make predictions\n",
        "pred = sgd_lr.predict_proba(x_test_enc.toarray())[:,1]"
      ],
      "metadata": {
        "id": "VaCYuHlXlv28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the roc score\n",
        "print(f'Training samples: {x_train.shape[0]}, AUC on testing set: {roc_auc_score(y_test,pred):.3f}')"
      ],
      "metadata": {
        "id": "YqMFZcjkmFPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#i will add regularization to the model\n",
        "#this regularizes the weight updates and is implemented with the penalty hyperparameter\n",
        "#The regularization term is introduced in order to penalize large weights,\n",
        "#as the weights now become part of the cost to minimize.\n",
        "#Regularization as a result eliminates overfitting. Finally, parameter 'α' provides a trade\n",
        "#off between log loss and generalization. If 'α' is too small, it is not able to compress\n",
        "#large weights and the model may suffer from high variance or overfitting; on the\n",
        "#other hand, if 'α' is too large, the model may become over-generalized and perform\n",
        "#poorly in terms of fitting the dataset, which is the syndrome of underfitting. α is an\n",
        "#important parameter to tune in order to obtain the best logistic regression model with\n",
        "#regularization."
      ],
      "metadata": {
        "id": "tYuxK0GTmOnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initialize with aplha and penalty term as l1\n",
        "sgd_lr_l1 = SGDClassifier(loss='log_loss',penalty='l1',alpha=0.0001,fit_intercept=True,max_iter=20,learning_rate='constant',eta0=0.01)\n",
        "\n",
        "#fit\n",
        "sgd_lr_l1.fit(x_train_enc.toarray(),y_train)"
      ],
      "metadata": {
        "id": "wIvKvArNnOUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##lets obtain the absolute values of coefficients\n",
        "absolute_coefs = np.abs(sgd_lr_l1.coef_)\n",
        "\n",
        "#print the shape\n",
        "print(absolute_coefs.shape)"
      ],
      "metadata": {
        "id": "_jqS9REZn1zz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the model when combined with the penalty=l1 and an alpha value\n",
        "##can be used as a feature selector because it tells which features is redundant\n",
        "#by pushing their coefficients close to or exaclty 0."
      ],
      "metadata": {
        "id": "ULbc64SVoTcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##a more robust method for feature selection is the Random Forest method\n",
        "##random forest is a form of bagging strategy over a set of decision trees\n",
        "##each tree considers a random subset of the features when searching for the best\n",
        "##splitting point at each node. IN decision tree, only those significant features are used to constitute tree nodes.\n",
        "\n",
        "#lets implement it\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "#initialize\n",
        "rf = RandomForestClassifier(n_estimators=100,criterion='gini',min_samples_split=30,n_jobs=-1)\n",
        "\n",
        "#fit\n",
        "rf.fit(x_train_enc.toarray(),y_train)"
      ],
      "metadata": {
        "id": "3N4tAZ2st7O9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##lets check feature importances\n",
        "f_imp = rf.feature_importances_\n",
        "\n",
        "print(f_imp)"
      ],
      "metadata": {
        "id": "WDDkQQdWu55w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##lets get features names from the encoder\n",
        "f_names = enc.get_feature_names_out()\n",
        "\n",
        "#attach the feature names to their importances\n",
        "print(np.sort(f_imp)[:10])"
      ],
      "metadata": {
        "id": "fA7BHU_yvBUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XombHoRKv7mi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}